# PPO è¯Šæ–­å­—æ®µå®Œæ•´æ€§æŠ¥å‘Š

## ğŸ“‹ å®æ–½æ€»ç»“

å·²åœ¨ PPO è®­ç»ƒæ—¥å¿—ä¸­æ·»åŠ å®Œæ•´çš„è¯Šæ–­æŒ‡æ ‡ï¼Œç”¨äºåˆ¤æ–­æ‹‰æ ¼æœ—æ—¥é¡¹æ˜¯å¦çœŸæ­£å½±å“ policy æ›´æ–°ã€‚

---

## âœ… å·²å®ç°çš„è¯Šæ–­å­—æ®µï¼ˆ14 ä¸ªå¿…éœ€å­—æ®µï¼‰

### A. Policy æ˜¯å¦è¿˜åœ¨åŠ¨ï¼ˆè¯Šæ–­2ï¼‰
- `approx_kl`: PPO policy æ›´æ–°çš„ KL æ•£åº¦è¿‘ä¼¼å€¼
  - è®¡ç®—æ–¹å¼: `mean((exp(log_ratio) - 1) - log_ratio)`
  - ä½ç½®: æ¯ä¸ª minibatch è®¡ç®—åå–å¹³å‡

### B. PPO æ›´æ–°å¼ºåº¦è¾…åŠ©
- `clip_frac`: PPO ratio è¢« clip çš„æ¯”ä¾‹
  - è®¡ç®—æ–¹å¼: `mean(|ratio - 1| > clip_coef)`
- `entropy`: Policy åˆ†å¸ƒçš„å¹³å‡ç†µ
  - æ¥æº: PPO åŸºç¡€ metricsï¼Œå·²å­˜åœ¨

### C. Cost æ˜¯å¦çœŸçš„åœ¨æ¨åŠ¨ Policyï¼ˆæ ¸å¿ƒè¯Šæ–­ï¼‰
æ‰€æœ‰ç»Ÿè®¡é‡åŸºäº **actor å®é™…ä½¿ç”¨çš„ normalized advantage**ï¼ˆä¸ policy loss å®Œå…¨ä¸€è‡´ï¼‰

- `adv_reward_abs_mean`: `mean(|adv_reward|)`
- `adv_penalty_abs_mean`: `mean(|penalty_adv_total|)`
- `adv_penalty_to_reward_ratio`: `penalty_abs / (reward_abs + 1e-8)`
- `adv_reward_mean`: `mean(adv_reward)` å¸¦ç¬¦å·
- `adv_penalty_mean`: `mean(penalty_adv)` å¸¦ç¬¦å·

**è®¡ç®—ç»†èŠ‚ï¼ˆseparate æ¨¡å¼ï¼‰:**
```python
penalty_adv_total = lambda_energy * adv_energy + lambda_load * adv_load
```

**è®¡ç®—ç»†èŠ‚ï¼ˆaggregated æ¨¡å¼ï¼‰:**
```python
penalty_adv_total = lambda_energy * adv_cost_total
```

### D. åˆ†çº¦æŸè´¡çŒ®ï¼ˆç²¾ç»†è¯Šæ–­ï¼‰
- `lambdaA_energy_abs_mean`: `mean(|lambda_energy * adv_energy|)`
- `lambdaA_load_abs_mean`: `mean(|lambda_load * adv_load|)`
- `lambdaA_total_abs_mean`: `mean(|penalty_adv_total|)`

**Aggregated æ¨¡å¼ç‰¹æ®Šå¤„ç†:**
- `lambdaA_energy_abs_mean` = 0.0
- `lambdaA_load_abs_mean` = 0.0
- `lambdaA_total_abs_mean` = `mean(|lambda_total * adv_cost_total|)`

### E. é¢å¤–å­—æ®µï¼ˆå·²ä¿ç•™ï¼‰
- `rho_energy`: å…¨å±€ç´¯è®¡æˆæœ¬ç‡ = `cumulative_cost_energy / total_steps`
- `rho_load`: å…¨å±€ç´¯è®¡æˆæœ¬ç‡ = `cumulative_cost_load / total_steps`

**è¯´æ˜:** rho_* æ˜¯ Safety Gym é£æ ¼çš„å…¨å±€ç´¯è®¡æŒ‡æ ‡ï¼Œä¸ C/D çš„é€ batch ä¼˜åŠ¿ç»Ÿè®¡äº’è¡¥ã€‚

---

## ğŸ“ ä»£ç å®ç°ä½ç½®

### 1. PPO Agent (`ppo_multi_agent.py`)

#### è®¡ç®—ä½ç½®
**å‡½æ•°:** `MultiCriticPPO.update()`  
**è¡Œå·:** ~530-576

åœ¨å®Œæˆ advantage å½’ä¸€åŒ–åã€è¿›å…¥ minibatch å¾ªç¯å‰è®¡ç®—æ‰€æœ‰è¯Šæ–­é‡ï¼š

```python
# Line ~530
adv_eff = self._normalize_advantage(adv_eff)

# Advantage diagnostics (use normalized advantages aligned with actor loss)
adv_penalty_metrics: Dict[str, float] = {}
lambdaA_metrics: Dict[str, float] = {}
with torch.no_grad():
    adv_reward_norm = torch.as_tensor(
        self._normalize_advantage(adv_reward), ...
    )
    # ... è®¡ç®—æ‰€æœ‰è¯Šæ–­é‡
```

#### å†™å…¥ metrics ä½ç½®
**å‡½æ•°:** `MultiCriticPPO.update()`  
**è¡Œå·:** ~1147-1172

```python
# Line ~1147
metrics["lambda_gap_mode"] = self.cfg.lambda_gap_mode

# [æ–°å¢ 4] å†™å…¥æ–°æŒ‡æ ‡
metrics.update(rho_metrics)
metrics["approx_kl"] = np.mean(approx_kls) if approx_kls else 0.0
metrics["clip_frac"] = np.mean(clip_fracs) if clip_fracs else 0.0
metrics.update(adv_penalty_metrics)
metrics.update(lambdaA_metrics)

# [Sanity Check] ç¬¬ä¸€æ¬¡ update æ—¶éªŒè¯
if self._iter_count == 1:
    diag_keys = [
        "approx_kl", "clip_frac", "entropy",
        "adv_reward_abs_mean", "adv_penalty_abs_mean", ...
    ]
    missing = [k for k in diag_keys if k not in metrics]
    if missing:
        print(f"[WARNING] Missing diagnostic keys: {missing}")
    else:
        print(f"[OK] All diagnostic keys present (iter={self._iter_count})")
```

### 2. è®­ç»ƒè„šæœ¬ (`train_grid_structured_lagrangian.py`)

#### å†™å…¥ log_entry ä½ç½®
**è¡Œå·:** ~1106-1131

```python
# Line ~1106
# [æ–°å¢] æ·»åŠ è¯Šæ–­æŒ‡æ ‡åˆ° log_entryï¼ˆåœ¨ logger.log ä¹‹å‰ï¼‰

# 1. Safety Gym é£æ ¼ç´¯è®¡æˆæœ¬ç‡
for key in metrics:
    if key.startswith("rho_"):
        log_entry[key] = metrics[key]

# 2. PPO æ›´æ–°è¯Šæ–­
if "approx_kl" in metrics:
    log_entry["approx_kl"] = metrics["approx_kl"]
if "clip_frac" in metrics:
    log_entry["clip_frac"] = metrics["clip_frac"]

# 3. Advantage penalty diagnosticsï¼ˆæ ¸å¿ƒè¯Šæ–­ï¼‰
for key in [
    "adv_reward_abs_mean",
    "adv_penalty_abs_mean",
    "adv_penalty_to_reward_ratio",
    "adv_reward_mean",
    "adv_penalty_mean",
    "lambdaA_energy_abs_mean",
    "lambdaA_load_abs_mean",
    "lambdaA_total_abs_mean",
]:
    if key in metrics:
        log_entry[key] = metrics[key]
```

#### æœ€ç»ˆå†™å…¥ metrics.json
**è¡Œå·:** ~1277

```python
logger.log(log_entry)  # åŒ…å«æ‰€æœ‰è¯Šæ–­å­—æ®µ
```

**ä¿å­˜ä½ç½®:** ~1471
```python
metrics_path = os.path.join(output_dir, "metrics.json")
logger.save(metrics_path)
```

---

## ğŸ” éªŒè¯æ–¹æ³•

### æ–¹æ³•1: ä½¿ç”¨éªŒè¯è„šæœ¬
```bash
python verify_diagnostic_fields.py outputs/four_group_ablation_20260121/A_multi_critic_adaptive_seed0/metrics.json
```

### æ–¹æ³•2: è¿è¡Œè®­ç»ƒå¹¶è§‚å¯Ÿè¾“å‡º
```bash
./run_group_a_only.bat
```

**æœŸæœ›è¾“å‡ºï¼ˆç¬¬ä¸€ä¸ª iterationï¼‰:**
```
[OK] All diagnostic keys present in metrics (iter=1)
```

### æ–¹æ³•3: æ‰‹åŠ¨æ£€æŸ¥ metrics.json
æ‰“å¼€ `outputs/.../metrics.json`ï¼Œæ£€æŸ¥ç¬¬ 2 ä¸ª iteration çš„ entry æ˜¯å¦åŒ…å«æ‰€æœ‰ 14 ä¸ªå¿…éœ€å­—æ®µã€‚

---

## âš ï¸ é‡è¦çº¦æŸï¼ˆå·²éµå®ˆï¼‰

âœ… **ä¸æ”¹å˜è®­ç»ƒé€»è¾‘**: æ‰€æœ‰è¯Šæ–­é‡åœ¨ `torch.no_grad()` å—ä¸­è®¡ç®—  
âœ… **ä¸å½±å“æ¢¯åº¦**: ä»…è¯»å– advantage è¿›è¡Œç»Ÿè®¡ï¼Œä¸ä¿®æ”¹ç”¨äº loss çš„å¼ é‡  
âœ… **å£å¾„ä¸€è‡´**: ä½¿ç”¨ä¸ actor loss å®Œå…¨ç›¸åŒçš„ normalized advantage  
âœ… **æ¯ iteration è®°å½•**: æ‰€æœ‰å­—æ®µåœ¨æ¯æ¬¡ `agent.update()` åå†™å…¥ metrics  
âœ… **æ¨¡å¼å…¼å®¹**: separate å’Œ aggregated æ¨¡å¼å‡å·²é€‚é…  

---

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### æ£€æŸ¥æ‹‰æ ¼æœ—æ—¥é¡¹æ˜¯å¦ç”Ÿæ•ˆ
```python
import json
import matplotlib.pyplot as plt

with open('outputs/.../metrics.json', 'r') as f:
    data = json.load(f)

iterations = [d['iteration'] for d in data]
ratio = [d.get('adv_penalty_to_reward_ratio', 0) for d in data]
lambda_e = [d.get('lambda_energy', 0) for d in data]

plt.plot(iterations, ratio, label='Penalty/Reward Ratio')
plt.plot(iterations, lambda_e, label='Lambda Energy', alpha=0.5)
plt.xlabel('Iteration')
plt.legend()
plt.show()
```

### åˆ¤æ–­ policy æ˜¯å¦åœæ»
```python
approx_kl = [d.get('approx_kl', 0) for d in data]
clip_frac = [d.get('clip_frac', 0) for d in data]

# è‹¥ approx_kl æŒç»­ < 1e-4 ä¸” clip_frac < 0.01ï¼Œè¯´æ˜ policy å‡ ä¹ä¸åŠ¨
```

---

## ğŸ¯ ä¸‹ä¸€æ­¥

1. **è¿è¡Œè®­ç»ƒ**: `./run_group_a_only.bat`
2. **éªŒè¯å­—æ®µ**: `python verify_diagnostic_fields.py <metrics_path>`
3. **åˆ†æç»“æœ**: ä½¿ç”¨ä¸Šè¿°ç¤ºä¾‹ä»£ç ç»˜åˆ¶è¯Šæ–­æ›²çº¿

æ‰€æœ‰å¿…éœ€å­—æ®µå·²å®Œæ•´å®ç°å¹¶å†™å…¥ metrics.jsonã€‚
